{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauage Processing\n",
    "The purpose of this notebook is to familiarize myself with concepts and common techniques in NLP:\n",
    "- nltk module\n",
    "- Tokenizing\n",
    "- Removing Stop words\n",
    "- Stemming\n",
    "- Part of Speech Tagging\n",
    "- Chunking and Chinking\n",
    "- Named Entity Recognition\n",
    "- Movie review classification\n",
    "\n",
    "##### Appendix:\n",
    "- More on Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit (nltk) module\n",
    "This is a well-known module that aids the handling of human language data. It contains functions to perform tasks such as tokenizing, stemming, tagging, and chunking.\n",
    "\n",
    "Part of a useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize sentences:  ['Hi, my name is inigo montoya.', 'You killed my father.', 'Prepare to die.']\n",
      "Tokenize words:  ['Hi', ',', 'my', 'name', 'is', 'inigo', 'montoya', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text = 'Hi, my name is inigo montoya. You killed my father. Prepare to die.'\n",
    "# setense tokenization\n",
    "sent_tok = sent_tokenize(text)\n",
    "print('Tokenize sentences: ', sent_tok)\n",
    "# word tokenization\n",
    "word_tok = word_tokenize(sent_tok[0])\n",
    "print('Tokenize words: ', word_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'computing',\n",
       " ',',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'words',\n",
       " 'filtered',\n",
       " 'processing',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sentence = 'In computing, stop words are words which are filtered out before or after processing of natural language data.'\n",
    "non_stop = [w for w in word_tokenize(sentence) if not w in set(stopwords.words('english'))]\n",
    "non_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat', 'ate', 'eaten', 'eat', 'eat']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = ['eat','ate','eaten','eating','eats']\n",
    "[ps.stem(w) for w in example_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'reduc',\n",
       " 'a',\n",
       " 'word',\n",
       " 'to',\n",
       " 'it',\n",
       " 'word',\n",
       " 'stem',\n",
       " 'that',\n",
       " 'affix',\n",
       " 'to',\n",
       " 'suffix',\n",
       " 'and',\n",
       " 'prefix',\n",
       " 'or',\n",
       " 'to',\n",
       " 'the',\n",
       " 'root',\n",
       " 'of',\n",
       " 'word',\n",
       " 'known',\n",
       " 'as',\n",
       " 'a',\n",
       " 'lemma',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.'\n",
    "word_tok = word_tokenize(text)\n",
    "[ps.stem(w) for w in word_tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging\n",
    "Before going into the tagging part, we can customize our sentence tokenizer so that it will perform better on our sample text. The algorithm to do this is the Punkt Sentence Tokenizer. Unlabeled text can be used to train the algorithm on splitting text into sentences (sentence tokenization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "test_text = state_union.raw('2006-GWBush.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punkt Sentence Tokenizer is an unsuervised learning algorithm that can learn \n",
    "# how to tokenize sentences (splitting a volume of text into sentences) using unlabeled text.\n",
    "punk = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing our tokenizer\n",
    "test_tok = punk.tokenize(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text of interest is tokenized by sentence, we can perform part of speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRESIDENT', 'NNP'),\n",
       " ('GEORGE', 'NNP'),\n",
       " ('W.', 'NNP'),\n",
       " ('BUSH', 'NNP'),\n",
       " (\"'S\", 'POS'),\n",
       " ('ADDRESS', 'NNP'),\n",
       " ('BEFORE', 'IN'),\n",
       " ('A', 'NNP'),\n",
       " ('JOINT', 'NNP'),\n",
       " ('SESSION', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('CONGRESS', 'NNP'),\n",
       " ('ON', 'NNP'),\n",
       " ('THE', 'NNP'),\n",
       " ('STATE', 'NNP'),\n",
       " ('OF', 'IN'),\n",
       " ('THE', 'NNP'),\n",
       " ('UNION', 'NNP'),\n",
       " ('January', 'NNP'),\n",
       " ('31', 'CD'),\n",
       " (',', ','),\n",
       " ('2006', 'CD'),\n",
       " ('THE', 'NNP'),\n",
       " ('PRESIDENT', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each sentence token, tokenize the words. Then, tag each word the the part of speech\n",
    "from nltk import pos_tag\n",
    "tagged = []\n",
    "for sent in test_tok:\n",
    "    words = word_tokenize(sent)\n",
    "    tagged.append(pos_tag(words))\n",
    "tagged[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "Chunking is to group words with nouns and their corresponding verbs, adjectives, or adverbs. To do so, we will use regular expression to find words groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression code\n",
    "'''\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\n",
    ". = Any character except a new line\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunks(chunked):\n",
    "    for sub in chunked.subtrees():\n",
    "        if sub.label() == 'Chunk':\n",
    "            print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n"
     ]
    }
   ],
   "source": [
    "# defining the grammars of each chunk\n",
    "# RB --> adverb (0 or more; any tense)\n",
    "# VB --> verb (0 or more; any tense)\n",
    "# NNP --> proper noun (1 or more)\n",
    "# NN --> noun (0 or 1)\n",
    "chunkGram = r'Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}'\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged[1])\n",
    "print_chunks(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinking\n",
    "A more complex form of chunking. When the syntax is given, it removes any groups of words from the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  Congress/NNP\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/VB\n",
      "  our/PRP$\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  a/DT\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  who/WP\n",
      "  called/VBD\n",
      "  America/NNP\n",
      "  to/TO\n",
      "  its/PRP$\n",
      "  founding/NN\n",
      "  ideals/NNS\n",
      "  and/CC\n",
      "  carried/VBD\n",
      "  on/IN\n",
      "  a/DT\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n",
      "###############\n",
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS)\n",
      "(Chunk Congress/NNP ,/, members/NNS)\n",
      "(Chunk\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:)\n",
      "(Chunk our/PRP$ nation/NN)\n",
      "(Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "(Chunk noble/JJ dream/NN ./.)\n"
     ]
    }
   ],
   "source": [
    "# Comparing with or without chinking\n",
    "#chunk every word\n",
    "chunkGram1 = r'Chunk: {<.*>+}'\n",
    "#chunk every word, then split the chunk by specific POS (VB,IN,DT,TO)\n",
    "chunkGram2 = r'''Chunk: {<.*>+}\n",
    "                        }<VB.?|IN|DT|TO>+{'''\n",
    "chunkParser1 = nltk.RegexpParser(chunkGram1)\n",
    "chunkParser2 = nltk.RegexpParser(chunkGram2)\n",
    "chunked1 = chunkParser1.parse(tagged[1])\n",
    "chunked2 = chunkParser2.parse(tagged[1])\n",
    "print_chunks(chunked1)\n",
    "print('###############')\n",
    "print_chunks(chunked2)\n",
    "chunked1.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name entity recognition is used to extract from unstructured text the\n",
    "# categories of entities such as person, organization, location, etc.\n",
    "from nltk import ne_chunk\n",
    "namedEnt_list = []\n",
    "# tagged is a list of tagged part of speech of words in a sentence\n",
    "for each_tag in tagged:\n",
    "    namedEnt_list.append(ne_chunk(each_tag, binary=False))\n",
    "namedEnt_list[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Movie review classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos/cv000_29590.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(films, adapted, from, comic, books, have, had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos/cv001_18431.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(every, now, and, then, a, movie, comes, along...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos/cv002_15918.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(you, ', ve, got, mail, works, alot, better, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos/cv003_11664.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(\", jaws, \", is, a, rare, film, that, grabs, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos/cv004_11636.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(moviemaking, is, a, lot, like, being, the, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pos/cv005_29443.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(on, june, 30, ,, 1960, ,, a, self, -, taught,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pos/cv006_15448.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(apparently, ,, director, tony, kaye, had, a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pos/cv007_4968.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(one, of, my, colleagues, was, surprised, when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pos/cv008_29435.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(after, bloody, clashes, and, independence, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pos/cv009_29592.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(the, american, action, film, has, been, slowl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pos/cv010_29198.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(after, watching, \", rat, race, \", last, week,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pos/cv011_12166.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(i, ', ve, noticed, something, lately, that, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pos/cv012_29576.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(synopsis, :, bobby, garfield, (, yelchin, ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pos/cv013_10159.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(synopsis, :, in, this, movie, ,, steven, spie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pos/cv014_13924.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(the, police, negotiator, is, the, person, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pos/cv015_29439.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(plot, :, a, young, man, who, loves, heavy, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pos/cv016_4659.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(carry, on, matron, is, the, last, great, carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pos/cv017_22464.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(the, ultimate, match, up, between, good, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pos/cv018_20137.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(having, not, seen, ,, \", who, framed, roger, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pos/cv019_14482.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(there, ', s, something, about, ben, stiller, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pos/cv020_8825.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(by, phil, curtolo, mel, gibson, (, braveheart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pos/cv021_15838.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(one, can, not, observe, a, star, trek, movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pos/cv022_12864.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(a, fully, loaded, entertainment, review, -, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pos/cv023_12672.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(when, bulworth, ended, ,, i, allowed, myself,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pos/cv024_6778.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(call, 911, for, the, cliche, police, if, you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pos/cv025_3108.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(hilarious, ,, ultra, -, low, budget, comedy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pos/cv026_29325.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(for, those, of, us, who, weren, ', t, yet, bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pos/cv027_25219.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(the, most, common, (, and, in, many, cases, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pos/cv028_26746.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(\", the, blair, witch, project, \", was, perhap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pos/cv029_18643.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>(seen, may, 31, ,, 1999, on, home, video, (, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>neg/cv970_19532.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(the, \", disney, stick, -, to, -, what, -, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>neg/cv971_11790.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(as, the, twin, surfer, dudes, ,, stew, and, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>neg/cv972_26837.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(', bicentennial, man, ', is, a, family, film,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>neg/cv973_10171.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(in, the, continuation, of, warner, brother, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>neg/cv974_24303.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(long, ago, ,, films, were, constructed, of, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>neg/cv975_11920.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(stars, :, armand, assante, (, mike, hammer, )...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>neg/cv976_10724.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(nostalgia, for, the, 70s, continues, ,, as, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>neg/cv977_4776.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(i, saw, this, film, on, christmas, day, expec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>neg/cv978_22192.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(anna, and, the, king, is, at, least, the, fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>neg/cv979_2029.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(robin, hood, :, men, in, tights, is, another,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>neg/cv980_11851.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(with, his, successful, books, and, movies, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>neg/cv981_16679.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(director, luis, mandoki, ', s, last, film, wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>neg/cv982_22209.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(i, remember, really, enjoying, this, movie, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>neg/cv983_24219.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(tommy, lee, jones, chases, an, innocent, vict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>neg/cv984_14006.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(while, i, am, not, fond, of, any, writer, ', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>neg/cv985_5964.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(boy, ,, what, a, great, movie, !, !, keanu, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>neg/cv986_15092.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(there, are, those, of, us, who, think, of, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>neg/cv987_7394.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(please, don, ', t, mind, this, windbag, letti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>neg/cv988_20168.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(this, movie, is, written, by, the, man, who, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>neg/cv989_17297.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(in, a, typical, cinematic, high, school, ,, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>neg/cv990_12443.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(man, ,, this, was, one, wierd, movie, ., simi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>neg/cv991_19973.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(the, king, and, i, ,, a, warner, brothers, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>neg/cv992_12806.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(synopsis, :, cro, -, magnon, ayla, loses, her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>neg/cv993_29565.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(salaries, of, hollywood, top, actors, are, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>neg/cv994_13229.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(movies, like, six, days, ,, seven, nights, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>neg/cv995_23113.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(if, anything, ,, \", stigmata, \", should, be, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>neg/cv996_12447.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(john, boorman, ', s, \", zardoz, \", is, a, goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>neg/cv997_5152.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(the, kids, in, the, hall, are, an, acquired, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>neg/cv998_15691.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(there, was, a, time, when, john, carpenter, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>neg/cv999_14636.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>(two, party, guys, bob, their, heads, to, hadd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  label  \\\n",
       "0     pos/cv000_29590.txt      1   \n",
       "1     pos/cv001_18431.txt      1   \n",
       "2     pos/cv002_15918.txt      1   \n",
       "3     pos/cv003_11664.txt      1   \n",
       "4     pos/cv004_11636.txt      1   \n",
       "5     pos/cv005_29443.txt      1   \n",
       "6     pos/cv006_15448.txt      1   \n",
       "7      pos/cv007_4968.txt      1   \n",
       "8     pos/cv008_29435.txt      1   \n",
       "9     pos/cv009_29592.txt      1   \n",
       "10    pos/cv010_29198.txt      1   \n",
       "11    pos/cv011_12166.txt      1   \n",
       "12    pos/cv012_29576.txt      1   \n",
       "13    pos/cv013_10159.txt      1   \n",
       "14    pos/cv014_13924.txt      1   \n",
       "15    pos/cv015_29439.txt      1   \n",
       "16     pos/cv016_4659.txt      1   \n",
       "17    pos/cv017_22464.txt      1   \n",
       "18    pos/cv018_20137.txt      1   \n",
       "19    pos/cv019_14482.txt      1   \n",
       "20     pos/cv020_8825.txt      1   \n",
       "21    pos/cv021_15838.txt      1   \n",
       "22    pos/cv022_12864.txt      1   \n",
       "23    pos/cv023_12672.txt      1   \n",
       "24     pos/cv024_6778.txt      1   \n",
       "25     pos/cv025_3108.txt      1   \n",
       "26    pos/cv026_29325.txt      1   \n",
       "27    pos/cv027_25219.txt      1   \n",
       "28    pos/cv028_26746.txt      1   \n",
       "29    pos/cv029_18643.txt      1   \n",
       "...                   ...    ...   \n",
       "1970  neg/cv970_19532.txt      0   \n",
       "1971  neg/cv971_11790.txt      0   \n",
       "1972  neg/cv972_26837.txt      0   \n",
       "1973  neg/cv973_10171.txt      0   \n",
       "1974  neg/cv974_24303.txt      0   \n",
       "1975  neg/cv975_11920.txt      0   \n",
       "1976  neg/cv976_10724.txt      0   \n",
       "1977   neg/cv977_4776.txt      0   \n",
       "1978  neg/cv978_22192.txt      0   \n",
       "1979   neg/cv979_2029.txt      0   \n",
       "1980  neg/cv980_11851.txt      0   \n",
       "1981  neg/cv981_16679.txt      0   \n",
       "1982  neg/cv982_22209.txt      0   \n",
       "1983  neg/cv983_24219.txt      0   \n",
       "1984  neg/cv984_14006.txt      0   \n",
       "1985   neg/cv985_5964.txt      0   \n",
       "1986  neg/cv986_15092.txt      0   \n",
       "1987   neg/cv987_7394.txt      0   \n",
       "1988  neg/cv988_20168.txt      0   \n",
       "1989  neg/cv989_17297.txt      0   \n",
       "1990  neg/cv990_12443.txt      0   \n",
       "1991  neg/cv991_19973.txt      0   \n",
       "1992  neg/cv992_12806.txt      0   \n",
       "1993  neg/cv993_29565.txt      0   \n",
       "1994  neg/cv994_13229.txt      0   \n",
       "1995  neg/cv995_23113.txt      0   \n",
       "1996  neg/cv996_12447.txt      0   \n",
       "1997   neg/cv997_5152.txt      0   \n",
       "1998  neg/cv998_15691.txt      0   \n",
       "1999  neg/cv999_14636.txt      0   \n",
       "\n",
       "                                                    rev  \n",
       "0     (films, adapted, from, comic, books, have, had...  \n",
       "1     (every, now, and, then, a, movie, comes, along...  \n",
       "2     (you, ', ve, got, mail, works, alot, better, t...  \n",
       "3     (\", jaws, \", is, a, rare, film, that, grabs, y...  \n",
       "4     (moviemaking, is, a, lot, like, being, the, ge...  \n",
       "5     (on, june, 30, ,, 1960, ,, a, self, -, taught,...  \n",
       "6     (apparently, ,, director, tony, kaye, had, a, ...  \n",
       "7     (one, of, my, colleagues, was, surprised, when...  \n",
       "8     (after, bloody, clashes, and, independence, wo...  \n",
       "9     (the, american, action, film, has, been, slowl...  \n",
       "10    (after, watching, \", rat, race, \", last, week,...  \n",
       "11    (i, ', ve, noticed, something, lately, that, i...  \n",
       "12    (synopsis, :, bobby, garfield, (, yelchin, ), ...  \n",
       "13    (synopsis, :, in, this, movie, ,, steven, spie...  \n",
       "14    (the, police, negotiator, is, the, person, wit...  \n",
       "15    (plot, :, a, young, man, who, loves, heavy, me...  \n",
       "16    (carry, on, matron, is, the, last, great, carr...  \n",
       "17    (the, ultimate, match, up, between, good, and,...  \n",
       "18    (having, not, seen, ,, \", who, framed, roger, ...  \n",
       "19    (there, ', s, something, about, ben, stiller, ...  \n",
       "20    (by, phil, curtolo, mel, gibson, (, braveheart...  \n",
       "21    (one, can, not, observe, a, star, trek, movie,...  \n",
       "22    (a, fully, loaded, entertainment, review, -, w...  \n",
       "23    (when, bulworth, ended, ,, i, allowed, myself,...  \n",
       "24    (call, 911, for, the, cliche, police, if, you,...  \n",
       "25    (hilarious, ,, ultra, -, low, budget, comedy, ...  \n",
       "26    (for, those, of, us, who, weren, ', t, yet, bo...  \n",
       "27    (the, most, common, (, and, in, many, cases, t...  \n",
       "28    (\", the, blair, witch, project, \", was, perhap...  \n",
       "29    (seen, may, 31, ,, 1999, on, home, video, (, r...  \n",
       "...                                                 ...  \n",
       "1970  (the, \", disney, stick, -, to, -, what, -, you...  \n",
       "1971  (as, the, twin, surfer, dudes, ,, stew, and, p...  \n",
       "1972  (', bicentennial, man, ', is, a, family, film,...  \n",
       "1973  (in, the, continuation, of, warner, brother, '...  \n",
       "1974  (long, ago, ,, films, were, constructed, of, s...  \n",
       "1975  (stars, :, armand, assante, (, mike, hammer, )...  \n",
       "1976  (nostalgia, for, the, 70s, continues, ,, as, w...  \n",
       "1977  (i, saw, this, film, on, christmas, day, expec...  \n",
       "1978  (anna, and, the, king, is, at, least, the, fou...  \n",
       "1979  (robin, hood, :, men, in, tights, is, another,...  \n",
       "1980  (with, his, successful, books, and, movies, ,,...  \n",
       "1981  (director, luis, mandoki, ', s, last, film, wa...  \n",
       "1982  (i, remember, really, enjoying, this, movie, w...  \n",
       "1983  (tommy, lee, jones, chases, an, innocent, vict...  \n",
       "1984  (while, i, am, not, fond, of, any, writer, ', ...  \n",
       "1985  (boy, ,, what, a, great, movie, !, !, keanu, r...  \n",
       "1986  (there, are, those, of, us, who, think, of, le...  \n",
       "1987  (please, don, ', t, mind, this, windbag, letti...  \n",
       "1988  (this, movie, is, written, by, the, man, who, ...  \n",
       "1989  (in, a, typical, cinematic, high, school, ,, t...  \n",
       "1990  (man, ,, this, was, one, wierd, movie, ., simi...  \n",
       "1991  (the, king, and, i, ,, a, warner, brothers, an...  \n",
       "1992  (synopsis, :, cro, -, magnon, ayla, loses, her...  \n",
       "1993  (salaries, of, hollywood, top, actors, are, ge...  \n",
       "1994  (movies, like, six, days, ,, seven, nights, ma...  \n",
       "1995  (if, anything, ,, \", stigmata, \", should, be, ...  \n",
       "1996  (john, boorman, ', s, \", zardoz, \", is, a, goo...  \n",
       "1997  (the, kids, in, the, hall, are, an, acquired, ...  \n",
       "1998  (there, was, a, time, when, john, carpenter, w...  \n",
       "1999  (two, party, guys, bob, their, heads, to, hadd...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from nltk.corpus import movie_reviews\n",
    "pos_ids = movie_reviews.fileids('pos') #this returns the file ids of all positive reviews\n",
    "# put them into a dataframe\n",
    "df_pos = pd.DataFrame()\n",
    "df_pos['id'] = pos_ids\n",
    "df_pos['label'] = [1]*len(pos_ids)\n",
    "# Same here for negative reviews\n",
    "neg_ids = movie_reviews.fileids('neg')\n",
    "df_neg = pd.DataFrame()\n",
    "df_neg['id'] = neg_ids\n",
    "df_neg['label'] = [0]*len(neg_ids)\n",
    "# concatenate the two dataframes. axis 0 means to append below\n",
    "data = pd.concat([df_pos,df_neg],ignore_index=True,axis=0)\n",
    "# obtain reviews using ids. Store in dataframe.\n",
    "data['rev'] = [movie_reviews.words(id) for id in data['id']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13914139a58>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomly shuffle the dataset\n",
    "data = data.sample(frac=1)\n",
    "# plot the labels to inspect distribution\n",
    "# looks like the data is very balanced.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data['label'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some data exploration\n",
    "this_rev = data.iloc[0]\n",
    "rev_words = [w.lower() for w in this_rev['rev']]\n",
    "freq_dist = nltk.FreqDist(rev_words)\n",
    "freq_dist['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all words into a list\n",
    "all_words = nltk.FreqDist([w for w in movie_reviews.words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# use the 4000 most frequent words as freatures\n",
    "common_words = list(all_words.keys())[:4000]\n",
    "# find_features searches for words in common_words that are present in each review\n",
    "def find_features(common_words,review):\n",
    "    # common_words is a list\n",
    "    # review is a list of words\n",
    "    review = [w.lower() for w in review]\n",
    "    return np.array([w in set(review) for w in common_words]).astype(int)\n",
    "find_features(common_words,data['rev'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate features matrix\n",
    "X_raw = np.array([find_features(common_words,review) for review in data['rev']])\n",
    "X_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4000)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the feature matrix\n",
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       242\n",
      "           1       0.85      0.83      0.84       258\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       500\n",
      "   macro avg       0.84      0.84      0.84       500\n",
      "weighted avg       0.84      0.84      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning! Split the dataset into training and testing\n",
    "# Use a support vector machine algorithm to classify\n",
    "# score the model performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw,data['label'])\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train,y_train)\n",
    "print(classification_report(model.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy:\n",
      "0.836\n",
      "Training accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# print out accuracy scores\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Testing accuracy:')\n",
    "print(accuracy_score(model.predict(X_test),y_test))\n",
    "print('Training accuracy:')\n",
    "print(accuracy_score(model.predict(X_train),y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like a lot of people, I loved Peter Jackson's original Lord of the Rings trilogy (although we can all admit Return of the King didn't quite know when to leave the party). So I was pretty surprised when Jackson took over from Guillermo del Toro to make the Hobbit trilogy, and the first film turned out to be such a boring mess. Even more so when The Desolation of Smaug rolled around, and the problems somehow seemed to get even worse. In what can only be described as the most honest promotional video of all time, we find out why: the movies were made completely on the fly, without a script or nearly any advanced planning. The above clip is from a behind-the-scenes video on the Battle of the Five Armies Blu-ray, and it features Peter Jackson, Andy Serkis, and other production personnel confessing that due to the director changeover â€” del Toro left the project after nearly two years of pre-production â€” Jackson hit the ground running but was never able to hit the reset button to get time to establish his own vision. In comparison, he spent years prepping the original Lord of the Rings trilogy, and on the Hobbit things got so bad that when they started shooting the titular Battle of Five Armies itself they were essentially just shooting B-roll: footage of people in costumes waving around swords, without any cohesive plan for how the sequence would actually play out. (A choice Jackson quote: \"I didn't know what the hell I was doing.\")They ended up postponing the shoot so Jackson could figure out what he actually wanted to make, but between the constant yawning and the sad resignation that never leaves his face throughout the clip, it's clear the movies never had a chance. I'm frankly shocked that any promotional clip would be this straightforward about the problems the film had, but hey â€” whatever gets people talking about the movie.\n"
     ]
    }
   ],
   "source": [
    "# A negative the hobbits review\n",
    "# Printing out the review\n",
    "hobbits = open(r'C:\\Users\\Chris\\Documents\\Data-Science-Projects\\Dataset\\The Hobbits review.txt','r').read()\n",
    "print(hobbits)\n",
    "hobbits = hobbits.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting if it is a positive or negative review\n",
    "# 0 means negative\n",
    "X_hobbits = find_features(common_words,hobbits)\n",
    "model.predict([X_hobbits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan Coogler's masterful superhero drama is unlike any other, featuring outstanding acting, breathtaking art direction, fascinating royal intrigue, memorable action sequences, and surprising depth. It's that depth -- of character, of storyline, of relevancy -- that makes Black Panther shine, as Boseman's T'Challa takes the mantle of king with enormous uncertainty about whether to share Wakanda's resources with the world. With the exception of his second-in-command W'Kabi (Kaluuya), T'Challa surrounds himself with an inner circle of influential women: Okoye, Nakia, his mother (Bassett), and his genius younger sister, scientist/tech inventor Shuri (Letitia Wright). Each of them contributes much to the story, with Gurira's spear-wielding Okoye the movie's clear scene-stealer, Wright the clever comic relief, and Nyong'o offering a wee bit of romance. Even the central villain, as played by frequent Coogler collaborator Jordan, is well-rounded and humanized, with the actor doing great work opposite the equally nuanced Boseman. There's so much to appreciate in Black Panther, from its pulsing score, which features a soundtrack overseen by award-winning rapper Kendrick Lamar, to the mesmerizing cinematography courtesy of DP Rachel Morrison, gorgeous tribal costumes, and vibrant production design. There's not as much laugh-aloud banter as viewers may have come to expect from Marvel movies, but the beats of humor that are here, usually thanks to plucky Shuri or mountain-tribe leader M'Baku (Winston Duke), are extra funny. Ultimately the film's success comes down to the thoughtful, compelling storytelling from director Coogler and writer Joe Robert Cole, as interpreted by a terrific cast of actors. This isn't just another highly entertaining but formulaic superhero story; it's also poignant and powerful and earns its place toward the top of Marvel's films. (Be sure to watch all the way through the credits for a couple of extra tidbits!)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A positive review on black panther\n",
    "# printing the review\n",
    "panther = open(r'C:\\Users\\Chris\\Documents\\Data-Science-Projects\\Dataset\\Black panther.txt','r').read()\n",
    "print(panther)\n",
    "panther = panther.split()\n",
    "X_panther = find_features(common_words,panther)\n",
    "# prediction is 1. That means the model predicted positive\n",
    "model.predict([X_panther])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: More on regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The '^' operator identifies the words starting with characters specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words starting with s:  ['share', 'serve', 'share', 'sovereign', 'stood']\n",
      "Words starting with th:  ['the', 'the', 'the', 'that', 'the']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = state_union.raw('2005-GWBush.txt')\n",
    "print('Words starting with s: ',[w for w in word_tokenize(text) if re.search('^s',w)][0:5])\n",
    "print('Words starting with th: ',[w for w in word_tokenize(text) if re.search('^th',w)][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The '$' operator identifies the words ending with characters specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ending with s:  ['members', 'Congress', 'citizens', 'As', 'Congress']\n"
     ]
    }
   ],
   "source": [
    "print('Words ending with s: ',[w for w in word_tokenize(text) if re.search('s$',w)][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The '+' operator means one or more repeating sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ending with one or more e:  ['Vice', 'the', 'share', 'free', 'see', 'free']\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in word_tokenize(text) if re.search('e+$',w)]\n",
    "print('Words ending with one or more ''e'': ',words[0:3]+[w for w in words if re.search('ee$',w)][0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The '*' operator means zero or more repeating sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words ending with zero or more e:  ['PRESIDENT', 'GEORGE', 'W.', 'BUSH', \"'S\"]\n"
     ]
    }
   ],
   "source": [
    "print('Words ending with zero or more e: ',[w for w in word_tokenize(text) if re.search('e*$',w)][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### '<>' specifies a token. And <.*> accepts everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['more', 'Americans'],\n",
       " ['more', 'flexible'],\n",
       " ['more', 'innovative'],\n",
       " ['more', 'competitive'],\n",
       " ['more', 'than']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <.*> accepts everything\n",
    "from nltk import TokenSearcher\n",
    "searcher = nltk.TokenSearcher(word_tokenize(text))\n",
    "searcher.findall(r'<more><.*>')[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.findall(r'<RB.?>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can find phrases containing sequences of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['guiding', 'ideal'],\n",
       " ['evening', 'I'],\n",
       " ['growing', 'economy'],\n",
       " ['going', 'back'],\n",
       " ['moving', 'into']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.findall(r'<.*ing><.*>')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is done following a tutorial by a class in Eduonix.\n",
    "The class can be found at https://www.eduonix.com/dashboard/learn-machine-learning-by-building-projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://www.nltk.org/\n",
    "- https://www.eduonix.com/dashboard/learn-machine-learning-by-building-projects\n",
    "- https://python.gotrained.com/nltk-regex/\n",
    "- https://en.wikipedia.org/wiki/Named-entity_recognition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
